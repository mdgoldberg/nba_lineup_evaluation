%!TEX root = ../thesis.tex

\chapter{Methods}
\label{ch:methods}

\newthought{At a high level, the goal of this analysis} is to determine how
effective a lineup will be, taking into account the talents and tendencies of all
ten players on the court. This involves two main steps: first, there must be a
quantitative representation of each player's abilities and play style. Then, these
``player profiles'' of the players on the court can be used to predict the outcome
of each possession, measured in terms of points scored. Therefore, the problem can
be stated as a regression with each possession as a unit of observation, points
scored as the response variable, and features describing the talents and play
styles of the players on the court during the possession as predictors.

A more in-depth discussion of the setup of the model is described in
section~\ref{sec:setup}. Then, the process of building player profiles to represent
each player-season is described in section~\ref{sec:profiles}. Next, the process of
reducing the dimensionality of these profiles is described in
section~\ref{sec:dim_red} and the regression techniques for predicting points in a
possession are described in section~\ref{sec:regress}. Finally, the manner in which
a model was selected and trained is described in section~\ref{sec:mod_sel}.

\section{Model Setup}
\label{sec:setup}

The goal of building player profiles for each player is to quantitatively represent
a player's play style in a given season. Play styles can change from season to
season due to factors like age and changes within a team, such as coaching changes
or roster moves through trades or free agency. Therefore, in order to quantify a
player's play style for season $t$, using statistics from within season $t$ is
essential. However, it is nonsensical to use play-by-play data from season $t$ to
construct player profiles if we are to use these player profiles to predict point
outcomes in season $t$ because the purpose of the model from the perspective of a
coach is to accurately predict point differential prior to observing these outcomes.
Therefore, one possible solution is to use play-by-play data from the first half of
season $t$ to build player profiles and then to use these player profiles to predict
point outcomes for each possession in the second half of season $t$. There are a few
advantages to this setup; first, it ensures that the player's current play style is
represented by using data from within the same season. Second, rookies can often
become important contributors to a team, so it is advantageous that this model that
can include such first-year players.

A second aspect of the model setup is that not every player in the league is given a
representation; instead, the number of plays (defined in chapter~\ref{ch:data}) for
which each player was on the court was computed, and the top 360 players were given
unique player profiles. The remaining players were binned into a ``replacement
players'' category, for which only one player profile was created using statistics
from all such players. The cutoff value of 360 was determined by using the fact that
there are 30 teams in the NBA and each team is allowed twelve active players on
their roster in any given game; this approach was borrowed from the
\citeauthor{Maymin} analysis. This aspect of the model is both a computational
convenience and a helpful addition to the model, for it allows comparison to a
baseline replacement player. For example, we can evaluate a player by predicting the
player's impact on the expected point differential between two teams of replacement
players; similarly, lineups can be evaluated by predicting the point differential
between the lineup and a lineup of replacement players. See chapter~\ref{ch:results}
for such analyses.

One issue with the setup so far is that a player that would otherwise be in the top
360 players by number of plays may be treated as a replacement player if they are
injured for the majority of the first half of the season. To avoid this issue, data
from season $t-1$ was included in addition to the first half of season $t$ in order
to generate a player profile for season $t$. In order to give more weight to more
recent data, statistics recorded in the first half of season $t$ were weighted by a
factor of 6. Because only half of season $t$ is used, this weighting scheme results
in roughly a 75\%/25\% split between statistics recorded in the first half of season
$t$ and statistics recorded in season $t-1$. As a result of this weighting scheme,
players who played very significant time in season $t-1$ would still be given a
unique player profile for possessions in the second half of season $t$, even if they
suffer injury for all of the first half of season $t$. To generate rate statistics,
this weighting scheme was used to find a weighted value for both the numerator and
the denominator of the rate statistic, rather than weighting the rates from each
season; this was done in order to avoid giving too much weight to either component
of the blend due to a disparity in sample sizes. For example, a weighted version of
free throw percentage for a player in season $t$ was computed as $(\text{FTM}_{t-1}
+ 6\text{FTM}_t)/(\text{FTA}_{t-1} + 6\text{FTA}_t)$, where FTA$_t$ is the number of
free throws a player attempted in season $t$ and FTM$_t$ is the number of free
throws a player made in season $t$.

\section{Building Player Profiles Representative of Play Style}
\label{sec:profiles}

In order to quantify a player's play style, I computed many rate statistics from the
play-by-play data described in chapter~\ref{ch:data}. It is important to note that
all statistics are rate statistics, as the goal is to capture the manner in which a
player plays, rather than the amount he plays. In order to take playing time out of
the equation, statistics were computed per play, where a play is essentially any
opportunity a team has to score (see chapter~\ref{ch:data}). While sample size can
normally be an issue with rate statistics, this concern is alleviated in this case
by the fact that all players not in the set of the 360 players with the most plays
were binned together when computing statistics such as plays, field goals, and
personal fouls.

In defining a player's play style, it is helpful to conceptualize the game of
basketball as consisting of three components: offense, defense, and rebounding.  Of
course, each of these has its own subcomponents, but this division is still helpful
for breaking down the game. For each of these aspects, I computed several rate
statistics that describe a player's play. In addition to these traditionally-defined
rate statistics in which a raw total of a certain event was divided by another, such
as field goal attempts per play, I compute offensive and defensive regularized
adjusted plus/minus ratings (ORAPM and DRAPM) for each player and for the
replacement player category using the model setup described in equation~\ref{eq:apm}
and estimating coefficients using a weighted version of the ridge regression
described in~\ref{eq:ridge_loss}. Specifically, possessions from the first half of
season $t$ are given six times the weight of possessions from season $t-1$; these
weights were applied by adding weights to the loss function for each training
sample, so coefficients were found by minimizing
\begin{equation} \label{eq:weighted_ridge_loss}
    \sum_{i=1}^n w_i \left( y_i - \bm{x_i}^T\bm{\beta} \right)^2 + \lambda
    \bm{\beta}^T\bm{\beta}
\end{equation}
where $w_i = 6$ if possession $i$ occurred in the first half of season $t$, and
$w_i = 1$ if possession $i$ occurred in season $t-1$. These ORAPM and DRAPM ratings
were meant to indicate general overall skill on each end of the floor, and to
capture the contributions a player can make that aren't captured by play-by-play
data.

Finally, after the player profiles were computed, they were standardized within each
year by subtracting the mean of a given profile feature within a given year and
dividing by the standard deviation. To clarify, a player profile for season $t$
includes data from season $t-1$ and the first half of season $t$; a weighted version
of each rate statistic is computed first, and standardization only comes after all
raw player profiles are computed. This has two justifications: first, it puts each
feature on the same scale; second, it controls for the changing landscape of the NBA
by comparing players to other players within a given year, rather than comparing
player-seasons that took place in very different contexts.

\begin{table}
    \centering
    \noindent\makebox[\textwidth]{%
    \begin{tabular}{cccc}
        \toprule
        Metric & Numerator & Denominator & Aspect of Play Style \\
        \midrule
        FGA/play & field goal attempts & offensive plays & scoring burden \\
        PF drawn/play & personal fouls & offensive plays & ability to draw fouls \\
        FTA/FGA & free throw attempts & field goal attempts & ability to get to the
        line \\
        FT\% & free throws made & free throw attempts & ability to convert shooting
        fouls \\
        \% of 2-pt FGM assisted & assisted 2-pt FGM & 2-pt FGM & tendency to create
        2-pt shots \\
        \% of 3-pt FGM assisted & assisted 3-pt FGM & 3-pt FGM & tendency to create
        3-pt shots \\
        \% of FGA by region & FGA from region & total FGA & distribution of FGA
        distances \\
        FG\% by region & FGM from region & FGA from region & efficiency from
        different distances \\
        AST/teammate FGM & assists & teammate FGM & tendency to facilitate \\
        Lost balls/play & lost balls & offensive plays & tendency to lose the ball
        \\
        Bad passes/play & bad passes & offensive plays & tendency to throw bad
        passes \\
        Travels/play & travels & offensive plays & tendency to travel \\
        Offensive fouls/play & offensive fouls & offensive plays & tendency to
        commit offensive fouls \\
        \midrule
        Block rate & blocks & opponent 2-pt FGA & tendency to protect the rim \\
        Steals/play & steals & defensive plays & ability to generate steals \\
        PF/play & personal fouls & defensive plays & tendency to commit
        defensive fouls \\
        Opponent TO/play & opponent turnovers & defensive plays & ability to
        generate turnovers \\
        \midrule
        Offensive rebounding rate & offensive rebounds & offensive rebound
        opportunities & ability to grab offensive boards \\
        Defensive rebounding rate & defensive rebounds & defensive rebound
        opportunities & ability to grab defensive boards \\
        \bottomrule
    \end{tabular}
    }
    \caption{Simple rate statistics collected to capture a player's play style.}
    \label{tab:features}
\end{table}

\subsection{Offensive Features}

A player's offensive game can be broken down into two main aspects: the manner in
which they score and the manner in which they handle the ball and avoid turnovers.
While there are other aspects of a player's offensive game, such as movement without
the ball, these aspects are difficult to measure using play-by-play data. In order
to measure a player's scoring play style, the following rate statistics were
collected: field goal attempts per play, free throw attempts per field goal attempt,
free throw percentage, percentage of made two-point field goals that were assisted,
percentage of made three-point field goals that were assisted, personal fouls drawn
per play, percentage of field goal attempts that come from various distances from
the basket, and field goal percentages from various distances from the basket. Each
of these is meant to indicate a slightly different aspect of the manner in which a
player scores; for example, field goal attempts per play is meant to measure the
degree to which a player shoulders the scoring burden for his team, and free throw
percentage is often used to measure a player's pure shooting ability in addition to
his ability to convert shooting fouls into points. For a description of each of
these statistics and what they intend to measure, see table~\ref{tab:features}.

The statistics describing how often and how efficiently a player shoots from various
distances require more explanation. The regions chosen, as well as justifications
for these choices, follow:

\begin{itemize}
    \item 2-point shots that are 0-4 feet from the basket. This roughly represents
        shots from the restricted area, which tend to be layups or shots out of the
        post.
    \item 2-point shots that are 5-8 feet from the basket. This region roughly
        represents the rest of the paint, as the paint extends eight feet from the
        basket towards the sideline in each direction. These shots tend to be
        floaters or shots from the post.
    \item 2-point shots 9-16 feet from the basket. This represents the traditional
        mid-range shot; note for context that the free throw line is 15 feet from
        the basket.
    \item 2-point shots that are 17 or more feet from the basket. These are longest
        two-point shots, and are generally considered the least efficient shots in
        basketball.
    \item 3-point shots that are 23 or fewer feet from the basket. These represent
        three-point shots from the corner, as the three-point line is 22 feet from
        the basket in the corners, but 23.75 feet from the basket as it arcs from
        one corner to the other.
    \item 3-point shots that are 24-26 feet from the basket. This range represents
        non-corner three-point shots.
    \item 3-point shots that are 27-30 feet from the basket. This range represents
        deep three-point attempts, a shot that is typically taken only when the shot
        or game clock is near zero or when a shooter has exceptional range. Shots
        longer than 30 feet are rare and are typically desperation heaves at the end
        of a quarter, which are not indicative of play style.
\end{itemize}

In order to measure a player's tendency to handle the ball and to avoid or cause
turnovers, the following rate statistics were collected: assists per teammate field
goal made while the player was on the court, lost balls per play, bad passes per
play, travels per play, and offensive fouls per play. The first statistic is a
measure of how much of a team's offense relies on a player as a facilitator when he
is on the court; it is important to note that teammate field goals made were only
counted when the player was on the court. The final four statistics are each a
disjoint component of turnovers per play; indeed, lost balls, bad passes, travels,
and offensive fouls are by far the four most frequent types of turnovers. This gives
a more granular look into how a player turns the ball over.

Finally, as described previously, a player's offensive RAPM was also used to
quantify a player's overall contribution to his team's offensive points per
possesion, in addition to the rate statistics quantifying one's scoring and
ball-handling.

\subsection{Defensive Features}

A common problem in quantifying a player's game is the lack of defensive statistics
that are recorded; this is mostly due to the fact that defense has relatively few
events that can be recorded. The main defense-related events that occur in the
play-by-play dataset are blocks, steals, personal fouls, and turnovers. Therefore,
the following rate statistics were collected to quantify defensive play styles:
blocks per opponent 2-point field goal attempt (also known as block rate), steals
per play, personal fouls per play, and opponent turnovers per play. It is important
to note that for block rate, opponent 2-point field goal attempts were only counted
if the player was on the court for the shot and thus had the opportunity to block
it; also, block rate is traditionally computed using only 2-point field goal
attempts because three-point shots are very rarely blocked. Likewise, opponent
turnovers were only considered for plays where the player was on the court. These
statistics are summarized in table~\ref{tab:features}.

\subsection{Rebounding Features}

Rebounding is a critical and often overlooked part of basketball that can make or
break a team. However, there are few statistics that effectively describe a player's
ability to rebound; here, only two rate statistics are gathered: offensive
rebounding rate and defensive rebounding rate. These are defined as the number of
offensive (or defensive) rebounds a player grabs per opportunity the player has to
grab an offensive (or defensive) rebound. In other words, defensive rebounding rate
is the total number of defensive rebounds a player grabs over the number of
defensive rebounds his team grabs while he's on the floor plus the number of
offensive rebounds the opposing team grabs while he's on the floor; offensive
rebounding rate is defined similarly. For completeness, these are also described in
table~\ref{tab:features}.

\section{Finding Latent Features via Dimensionality Reduction}
\label{sec:dim_red}

Player profiles comprised of the statistics described above were then passed through
various dimensionality reduction algorithms in order to find latent features that
describe a player's play style. This was done in order to make the features more
succinct, in that dimensionality would be reduced, and to reduce multicollinearity,
as many of the statistics above are correlated with one another. Several
dimensionality reduction techniques, both linear and nonlinear, were tested in order
to minimize cross-validation error on held-out data in the subsequent regression
(see section~\ref{sec:regress}). Specifically, principal component analysis (PCA),
Isomap, and locally linear embedding (LLE) were all used to reduce the
dimensionality of the player profiles.

First, PCA is a well-known linear dimensionality reduction technique that generates
linearly independent variables, called principal components, from the correlated
player profiles. The first principal component is chosen such that it maximizes the
amount of variance in the data that it explains; then, each subsequent principal
component is chosen to maximize the amount of variance explained under the condition
that it is orthogonal to each of the previous principal components \cite{PCA}.

Second, Isomap is a nonlinear dimensionality reduction technique that seeks to
respect geodesic distances between points. The algorithm connects points that are
near one another with edges and then computes the shortest path between any two
points over these edges using a shortest-path algorithm such as Dijkstra's algorithm
or the Floyd-Warshall algorithm. These geodesic distances are then used to form a
distance matrix; the $n$ eigenvectors of this distance matrix with the greatest
eigenvalues are then used as the coordinates for the new $n$-dimensional space
\cite{Isomap}.

Finally, LLE seeks to reduce dimension while preserving local distances within
neighborhoods of points. The LLE algorithm is similar to Isomap; like Isomap, it
constructs a graph of nearest neighbors by connecting neighbors with edges. Then, a
weight matrix is constructed by solving a linear system for the local neighborhood
defined for each point as the neighbors to which it is connected. Finally, like
Isomap, the top $n$ eigenvectors of this weight matrix are the coordinates for the
new $n$-dimensional space \cite{LLE}.

All three of these techniques have different strengths, and it is almost impossible
to identify when one technique will work and the other will not without very good
knowledge of the structure and geometry of the data. Without such information about
the player profiles, the best way to select a dimensionality reduction technique is
to test each one and choose the model that gives the best performance on
out-of-sample data; this is done via cross-validation and is described in
section~\ref{sec:mod_sel}.

\section{Predicting Point Differential Based on Lineup Composition}
\label{sec:regress}

Once player profiles were passed through a dimensionality reduction algorithm, they
were used to construct the design matrix for the regression predicting point
outcomes on each possession. Specifically, the player profiles were incorporated
into the regression via the following model:

\begin{equation} \label{eq:model_spec}
    y_i = f\left( a_{o_{1,i}}, \dots, a_{o_{5,i}}, b_{d_{1,i}}, \dots, b_{d_{5,i}},
    \bm{x_{o_{1,i}}}, \dots, \bm{x_{o_{5,i}}}, \bm{x_{d_{1,i}}}, \dots,
    \bm{x_{d_{5,i}}}, h_i \right) + \epsilon_i
\end{equation}

where $y_i$ is points scored by the offense on possession $i$, $o_{j,i}$ is the
$j$th best offensive player on the floor for possession $i$ when ranked by total
RAPM, $d_{j,i}$ is the $j$th best defensive player on the floor for possession $i$
when ranked by total RAPM, $a_k$ is the ORAPM rating for player $k$, $b_k$ is the
DRAPM rating for player $k$, and $\bm{x_k}$ is the latent player profile for player
$k$. Finally, $h_i$ is an indicator for whether the home team is on offense on
possession $i$ and $\epsilon_i$ is a zero-mean error term. The function $f$
represents some regression function that takes its inputs as predictors and models
the response variable $y_i$. In other words, the predictors are the ORAPM ratings of
the five offensive players, the DRAPM ratings of the five players on defense, and
the dimensionality-reduced player profiles for each of the ten players on the court,
in addition to an indicator for home court advantage. In order to impose an ordering
on the players, both the offensive and defensive lineups were ordered by total RAPM.

Several possible choices for the regression model $f$ were considered, including
both linear and nonlinear regression techniques. Ultimately, linear regression,
random forests \cite{RF}, and gradient boosting \cite{GB} were all tested as
regressors during cross-validation. Note that, although linear regression is, of
course, a linear model, it does account for interactions between play styles to an
extent due to the way the model is formulated. Because players are ordered by total
RAPM, substituting one player for another is not as simple as the RAPM approach of
adding the difference between the players' ratings; this is because a substitution
could result in a different order after being sorted by RAPM, which would result in
different predictors being matched with different coefficients. The final model was
selected based on performance in cross-validation, as explained in
section~\ref{sec:mod_sel}.

\section{Model Selection}
\label{sec:mod_sel}

The second half of each season contains roughly 114,000 possessions. Because of the
sheer quantity of data, cross-validation over the entire dataset would be very time-
and memory-consuming. Because we wish to test each possible combination of the
dimensionality reduction techniques mentioned in section~\ref{sec:dim_red} and the
regression techniques mentioned in section~\ref{sec:regress}, in addition to several
different hyperparameter settings for each of these techniques that can drastically
change performance, it would be infeasible to do cross-validation on the entire
ten-year dataset. In order to select the best model more efficiently,
cross-validation was done on three seasons of data, starting with the 2011-12 NBA
season and ending with the 2013-14 NBA season.  Three-fold cross-validation was
performed for each combination of dimensionality reduction technique and regression
technique, as well as for various different hyperparameter settings for each; in
particular, cross-validation was done so that in each fold, data from two of the
three seasons were used to train the regression model and the third year was held
out as a validation set on which to evaluate the model. Models were evaluated using
root-mean-square error (RMSE), and the model was selected by choosing the model with
the lowest cross-validation RMSE score. Once this model was selected, it was trained
on data starting with the 2006-07 season and ending with the 2013-14 season; the
last two seasons in the dataset were held out in order to determine the test RMSE on
unseen data. The results of this process are summarized in chapter~\ref{ch:results}.
