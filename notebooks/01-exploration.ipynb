{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the code used to gather the data and process it can be found at "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import operator as op\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import cluster, metrics, decomposition, mixture, preprocessing\n",
    "\n",
    "from sportsref import nba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5822, 27)\n"
     ]
    }
   ],
   "source": [
    "# load in data\n",
    "df = pd.read_csv('../data/interim/bkref_season_data_2001_2016.csv')\n",
    "df['player_name'] = df['player_id'].map(lambda p_id: nba.Player(p_id).name())\n",
    "data = df.iloc[:, 3:-1].values\n",
    "print data.shape\n",
    "normed = preprocessing.scale(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting with K-Means\n",
    "\n",
    "To start, I just used K-Means on normalized data, trying different values of K and evaluating them using [silhouette scores](https://en.wikipedia.org/wiki/Silhouette_(clustering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "def kmeans_find_k(data, start_k=3, end_k=15):\n",
    "    km_sils = {}\n",
    "    for nc in range(start_k, end_k + 1):\n",
    "        print nc\n",
    "        km = cluster.KMeans(n_clusters=nc, n_init=5, max_iter=200)\n",
    "        labels = km.fit_predict(data)\n",
    "        km_sils[nc] = metrics.silhouette_score(data, labels)\n",
    "    return km_sils\n",
    "\n",
    "km_sils = kmeans_find_k(normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 0.16164929081086116), (4, 0.1372948220171657), (5, 0.12732277811140427), (6, 0.095791977173414061), (8, 0.095494816671161872), (7, 0.095393611076950616), (9, 0.091912349375321736), (10, 0.086456831636446518), (11, 0.084369498938885193), (12, 0.080515334484060397), (13, 0.076128434177946111), (14, 0.072885849759601606), (15, 0.071417795944251947)]\n"
     ]
    }
   ],
   "source": [
    "print sorted(km_sils.items(), key=op.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exemplars for Cluster 0:\n",
      "Tyson Chandler    15\n",
      "Reggie Evans      13\n",
      "Ben Wallace       12\n",
      "Dwight Howard     12\n",
      "Chris Andersen    12\n",
      "Name: player_name, dtype: int64\n",
      "\n",
      "Exemplars for Cluster 1:\n",
      "Steve Nash       13\n",
      "Mo Williams      13\n",
      "Steve Blake      12\n",
      "Jameer Nelson    12\n",
      "Earl Watson      12\n",
      "Name: player_name, dtype: int64\n",
      "\n",
      "Exemplars for Cluster 2:\n",
      "Kobe Bryant     15\n",
      "Paul Pierce     14\n",
      "Tony Parker     14\n",
      "Dwyane Wade     13\n",
      "LeBron James    13\n",
      "Name: player_name, dtype: int64\n",
      "\n",
      "Exemplars for Cluster 3:\n",
      "Elton Brand        13\n",
      "Carlos Boozer      12\n",
      "Udonis Haslem      11\n",
      "Nazr Mohammed      10\n",
      "Antonio McDyess    10\n",
      "Name: player_name, dtype: int64\n",
      "\n",
      "Exemplars for Cluster 4:\n",
      "Dirk Nowitzki    16\n",
      "Pau Gasol        14\n",
      "Kevin Garnett    13\n",
      "David West       12\n",
      "Chris Bosh       12\n",
      "Name: player_name, dtype: int64\n",
      "\n",
      "Exemplars for Cluster 5:\n",
      "Mike Miller        14\n",
      "Kyle Korver        13\n",
      "Mike Dunleavy      12\n",
      "Tayshaun Prince    12\n",
      "Rasual Butler      11\n",
      "Name: player_name, dtype: int64\n",
      "\n",
      "Exemplars for Cluster 6:\n",
      "Tony Allen          12\n",
      "Andrei Kirilenko    11\n",
      "Gerald Wallace      11\n",
      "Jared Jeffries      10\n",
      "Matt Barnes         10\n",
      "Name: player_name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def kmeans_print_exemplars(data, n_clusters):\n",
    "    km = cluster.KMeans(n_clusters=n_clusters)\n",
    "    labels = km.fit_predict(data)\n",
    "    for clust in range(7):\n",
    "        print '\\nExemplars for Cluster {}:'.format(clust)\n",
    "        print df.groupby(labels).get_group(clust).player_name.value_counts().head(5)\n",
    "        \n",
    "kmeans_print_exemplars(normed, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it appears that on first blush using K-Means, around 7-10 clusters is the most reasonable number; while lower values of $K$ have greater silhouette values, there is a tradeoff between expressiveness (i.e., more clusters is able to differentiate more players) and model fit.\n",
    "\n",
    "## Dimensionality Reduction using PCA\n",
    "\n",
    "I decided to try dimensionality reduction to reduce the dimensionality of the data from 27 features to something more manageable, especially because many of these features are likely correlated with one another. While there are other methods of dimensionality reduction, to get started I decided to use PCA, since it is relatively easy out of the box with few parameters to tune, and it is able to take care of the multicolinearity problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.33379402,  0.45186612,  0.53920516,  0.59869022,  0.6476775 ,\n",
       "        0.68591681,  0.72094212,  0.75078579,  0.7802032 ,  0.80846164])"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = decomposition.PCA()\n",
    "transformed = pca.fit_transform(normed)\n",
    "np.cumsum(pca.explained_variance_ratio_[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "pca_data = transformed[:, :10]\n",
    "pca_km_sils = kmeans_find_k(pca_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 0.21217426474164144), (4, 0.18236505963482821), (5, 0.16057371836103665), (6, 0.14985542146347838), (7, 0.12385371480557451), (8, 0.12106984177569791), (9, 0.11714714219981233), (10, 0.11691441086243763), (11, 0.10755311438752425), (13, 0.10721878460071965), (14, 0.10465963931985155), (12, 0.10401660955726631), (15, 0.094498712625480721)]\n"
     ]
    }
   ],
   "source": [
    "print sorted(pca_km_sils.items(), key=op.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exemplars for Cluster 0:\n",
      "Kobe Bryant         15\n",
      "Paul Pierce         14\n",
      "Tony Parker         14\n",
      "Richard Hamilton    13\n",
      "Dwyane Wade         13\n",
      "Name: player_name, dtype: int64\n",
      "\n",
      "Exemplars for Cluster 1:\n",
      "Dirk Nowitzki    16\n",
      "Tim Duncan       14\n",
      "Pau Gasol        14\n",
      "Kevin Garnett    14\n",
      "David West       12\n",
      "Name: player_name, dtype: int64\n",
      "\n",
      "Exemplars for Cluster 2:\n",
      "Kyle Korver        13\n",
      "Mike Miller        11\n",
      "Tayshaun Prince    11\n",
      "Rasual Butler      11\n",
      "Peja Stojakovic    10\n",
      "Name: player_name, dtype: int64\n",
      "\n",
      "Exemplars for Cluster 3:\n",
      "Steve Nash       14\n",
      "Jameer Nelson    12\n",
      "Earl Watson      12\n",
      "Steve Blake      12\n",
      "Baron Davis      12\n",
      "Name: player_name, dtype: int64\n",
      "\n",
      "Exemplars for Cluster 4:\n",
      "Tony Allen          11\n",
      "Gerald Wallace      11\n",
      "Matt Barnes         10\n",
      "Andrei Kirilenko     9\n",
      "Thabo Sefolosha      9\n",
      "Name: player_name, dtype: int64\n",
      "\n",
      "Exemplars for Cluster 5:\n",
      "Elton Brand        13\n",
      "Udonis Haslem      11\n",
      "Antonio McDyess    10\n",
      "Carlos Boozer      10\n",
      "Chris Kaman         9\n",
      "Name: player_name, dtype: int64\n",
      "\n",
      "Exemplars for Cluster 6:\n",
      "Tyson Chandler    15\n",
      "Reggie Evans      13\n",
      "Ben Wallace       12\n",
      "Chris Andersen    12\n",
      "Dwight Howard     12\n",
      "Name: player_name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "pca = decomposition.PCA(n_components=10)\n",
    "transformed = pca.fit_transform(normed)\n",
    "\n",
    "kmeans_print_exemplars(transformed, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These clusters appear to be largely the same as without applying PCA; so on the bright side, it appears much of the lost variance was unimportant, so the dimensionality reduction was helpful and effective.\n",
    "\n",
    "## Clustering with a Gaussian Mixture Model\n",
    "\n",
    "Another clustering method is the [Gaussian Mixture Model](https://en.wikipedia.org/wiki/Mixture_model), which models each cluster as a multivariate Gaussian with some prior probability of a point coming from each cluster. This method is compelling because it is a generative probabilistic model, so we are able to evaluate the likelihood of the data under a given model, and then we can sample synthetic data from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "def gmm_find_k(data, start_k=3, end_k=15):\n",
    "    gmm_sils = {}\n",
    "    for nc in range(start_k, end_k + 1):\n",
    "        print nc\n",
    "        gmm = mixture.GaussianMixture(n_components=nc, max_iter=200)\n",
    "        gmm.fit(data)\n",
    "        labels = gmm.predict(data)\n",
    "        gmm_sils[nc] = metrics.silhouette_score(data, labels)\n",
    "    return gmm_sils\n",
    "\n",
    "gmm_sils = gmm_find_k(normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 0.19120643200156609), (4, 0.12341558900189001), (6, 0.1013001474170675), (5, 0.099743666990596377), (7, 0.078142718892079549), (8, 0.059834239059796443), (9, 0.04863120987641751), (12, 0.04796486046510777), (10, 0.041739598414048486), (11, 0.036601014952254235), (14, 0.024451346720371837), (15, 0.02144635462305923), (13, 0.018645525819344602)]\n"
     ]
    }
   ],
   "source": [
    "print sorted(gmm_sils.items(), key=op.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
